{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb752c6a",
   "metadata": {},
   "source": [
    "## I. Theoretical formulation of the problem\n",
    "\n",
    "Let us first have count table for collocates $C$ where $c_{u,v}$ denotes the amount of $(u,v)$ pairs in the training corpus. Then our task is to predict the probability that a new collocate pair starting with $u$ is $(u, v)$ or alternatively rank words $v_1,\\ldots, v_n$ such that the probability of $(u, v_i)$ is always greater than the pair $(u, v_{i+j})$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be601f7d",
   "metadata": {},
   "source": [
    "## II. Solution through Latent Dirihlet allocation\n",
    "\n",
    "Latent Dirichlet allocation (LDA) is a probabilistic model with hyper-parameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$ which originally explains occurences of terms in independent documents. We generalise it for collocations by splitting collocation pairs $(u,v)$ into disjoint groups $\\mathcal{G}_{u_1},\\ldots\\mathcal{G}_{u_n}$ where the group $\\mathcal{G}_u$ contains only collocates pairs $(u,v)$.\n",
    "\n",
    "Strictly speaking the collocate groups $\\mathcal{G}_{u_i}$ and $\\mathcal{G}_{u_j}$ do not correspond to the independent documents assumed by the LDA model, as some collocate pairs are in the same sentence or can have indirect influence on the following collacate pairs. However, this influence is very weak and the idenpence assumption is valid approximation.    \n",
    "\n",
    "To match the LDA model further, we must assume that all potential collocates $v_1,\\ldots, v_m$ can be split into $k$ semantical groups (topics) that might overlap. \n",
    "Each topic $T_z$ is modelled as a multinomial distribution determined by the parameter matrix $\\boldsymbol{\\beta}$. The parameter vector $\\boldsymbol{\\alpha}$ determines the topic probabilities.\n",
    "\n",
    "The corresponding generative model is following. To generate the collocate group $\\mathcal{G}_u$, we first sample the vector of topic probabilities \n",
    "\\begin{align*}\n",
    "\\Theta_u\\sim Dir(\\boldsymbol{\\alpha})\n",
    "\\end{align*}\n",
    "After that we are going to sample each new collocate pair by first sampling the topic $z$ and then sampling the collocate $v$ from the topic distribution:\n",
    "\\begin{align*}\n",
    "z&\\sim MultiNomial(\\Theta_u)\\\\\n",
    "v&\\sim MultiNomial(\\boldsymbol{\\beta}_{z})\n",
    "\\end{align*}\n",
    "\n",
    "Under these assumptions LDA training algorithm tries to find values of hyperparameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$ that maximises the marginal log-likelihood of the data. How this is achieved is irrelevant.\n",
    "\n",
    "For a fixed $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$ we can ask what is the posterior probability distribution of topics for each group $\\mathcal{G}_u$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[z|\\boldsymbol{\\alpha},\\boldsymbol{\\beta}, C]\n",
    "\\end{align*}\n",
    "Again the exact way how this is computed is irrelevant. It is important to note that the resulting posterior must be a multinomial distribution, as there are only k topics and a posterior must assign some value for each topic. In practice, this is the well documented inference procedure for any LDA algorithm.  \n",
    "\n",
    "\n",
    "As a result we know the posterior topic distribution $\\hat{\\Theta}_u$ for each group $\\mathcal{G}_u$ and now have to answer the question what is the probability of a new collocation pair $(u,v)$.\n",
    "\n",
    "According to the generative model \n",
    "\\begin{align*}\n",
    "\\Pr[v|\\boldsymbol{\\alpha},\\boldsymbol{\\beta}, C]= \\sum_z \\Pr[v, z|\\boldsymbol{\\alpha},\\boldsymbol{\\beta}, C] =\n",
    "\\sum_z \\Pr[v| z]\\cdot \\Pr[z|\\boldsymbol{\\alpha},\\boldsymbol{\\beta}, C]\n",
    "=\\sum_z \\beta_{z,v}\\cdot \\hat{\\Theta}_u\n",
    "\\end{align*}\n",
    "where $\\hat{\\Theta}_u$ are topic probabilities for the group $\\mathcal{G}_u$ and $\\beta_{z,v}$ is the probability that word $v$ is generated by the topic $z$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78903df9",
   "metadata": {},
   "source": [
    "material\n",
    "https://www.cs.cmu.edu/~mgormley/courses/10701-f16/slides/lecture20-topic-models.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
