import os
import pytest

from estnltk import Text
from estnltk.converters import dict_to_layer, layer_to_dict
from estnltk_neural.taggers import BertTokens2WordsRewriter


def test_bert_tokens_2_words_rewriter():
    # Case 1: bert tokens layer is a simple layer
    text = Text('RMK keelas Nipernaadil Rakvere linnametsa siseneda. '+\
                'Myanmari mässulised olid ka kuulnud Bangkokis toimuvast. '+\
                'See sundis MacArthurit lahkuma Kenyast ja siirduma Mecklenburgi. ')
    bert_tokens_layer = \
        {'ambiguous': False,
         'attributes': (),
         'enveloping': None,
         'meta': {},
         'name': 'bert_tokens',
         'parent': None,
         'secondary_attributes': (),
         'serialisation_module': None,
         'spans': [{'annotations': [{}], 'base_span': (0, 1)},
                   {'annotations': [{}], 'base_span': (1, 2)},
                   {'annotations': [{}], 'base_span': (2, 3)},
                   {'annotations': [{}], 'base_span': (4, 10)},
                   {'annotations': [{}], 'base_span': (11, 13)},
                   {'annotations': [{}], 'base_span': (13, 18)},
                   {'annotations': [{}], 'base_span': (18, 22)},
                   {'annotations': [{}], 'base_span': (23, 30)},
                   {'annotations': [{}], 'base_span': (31, 36)},
                   {'annotations': [{}], 'base_span': (36, 41)},
                   {'annotations': [{}], 'base_span': (42, 50)},
                   {'annotations': [{}], 'base_span': (50, 51)},
                   {'annotations': [{}], 'base_span': (52, 54)},
                   {'annotations': [{}], 'base_span': (54, 56)},
                   {'annotations': [{}], 'base_span': (56, 60)},
                   {'annotations': [{}], 'base_span': (61, 66)},
                   {'annotations': [{}], 'base_span': (66, 71)},
                   {'annotations': [{}], 'base_span': (72, 76)},
                   {'annotations': [{}], 'base_span': (77, 79)},
                   {'annotations': [{}], 'base_span': (80, 87)},
                   {'annotations': [{}], 'base_span': (88, 94)},
                   {'annotations': [{}], 'base_span': (94, 97)},
                   {'annotations': [{}], 'base_span': (98, 107)},
                   {'annotations': [{}], 'base_span': (107, 108)},
                   {'annotations': [{}], 'base_span': (109, 112)},
                   {'annotations': [{}], 'base_span': (113, 119)},
                   {'annotations': [{}], 'base_span': (120, 123)},
                   {'annotations': [{}], 'base_span': (123, 124)},
                   {'annotations': [{}], 'base_span': (124, 126)},
                   {'annotations': [{}], 'base_span': (126, 129)},
                   {'annotations': [{}], 'base_span': (129, 131)},
                   {'annotations': [{}], 'base_span': (132, 139)},
                   {'annotations': [{}], 'base_span': (140, 143)},
                   {'annotations': [{}], 'base_span': (143, 145)},
                   {'annotations': [{}], 'base_span': (145, 147)},
                   {'annotations': [{}], 'base_span': (148, 150)},
                   {'annotations': [{}], 'base_span': (151, 157)},
                   {'annotations': [{}], 'base_span': (157, 159)},
                   {'annotations': [{}], 'base_span': (160, 162)},
                   {'annotations': [{}], 'base_span': (162, 164)},
                   {'annotations': [{}], 'base_span': (164, 167)},
                   {'annotations': [{}], 'base_span': (167, 172)},
                   {'annotations': [{}], 'base_span': (172, 173)}]}
    text.add_layer( dict_to_layer(bert_tokens_layer) )
    words_layer = \
        {'ambiguous': True,
         'attributes': ('normalized_form',),
         'enveloping': None,
         'meta': {},
         'name': 'words',
         'parent': None,
         'secondary_attributes': (),
         'serialisation_module': None,
         'spans': [{'annotations': [{'normalized_form': None}], 'base_span': (0, 3)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (4, 10)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (11, 22)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (23, 30)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (31, 41)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (42, 50)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (50, 51)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (52, 60)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (61, 71)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (72, 76)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (77, 79)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (80, 87)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (88, 97)},
                   {'annotations': [{'normalized_form': None}], 'base_span': (98, 107)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (107, 108)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (109, 112)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (113, 119)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (120, 131)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (132, 139)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (140, 147)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (148, 150)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (151, 159)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (160, 172)},
                   {'annotations': [{'normalized_form': None}],
                    'base_span': (172, 173)}]}
    text.add_layer( dict_to_layer(words_layer) )
    
    def rewriter_decorator(text_obj, sharing_words, shared_bert_tokens):
        return {'bert_tokens': [t.text for t in shared_bert_tokens]}
    
    rewriter = BertTokens2WordsRewriter('bert_tokens', 
                                        input_words_layer = 'words', 
                                        output_attributes = ('bert_tokens',), 
                                        output_layer = 'bert_tokens_of_words',
                                        decorator = rewriter_decorator)
    rewriter.tag(text)
    #from pprint import pprint
    #pprint( layer_to_dict(text['bert_tokens_of_words']) )
    
    expected_output_layer = \
        {'ambiguous': False,
         'attributes': ('bert_tokens',),
         'enveloping': 'words',
         'meta': {},
         'name': 'bert_tokens_of_words',
         'parent': None,
         'secondary_attributes': (),
         'serialisation_module': None,
         'spans': [{'annotations': [{'bert_tokens': ['R', 'M', 'K']}],
                    'base_span': ((0, 3),)},
                   {'annotations': [{'bert_tokens': ['keelas']}],
                    'base_span': ((4, 10),)},
                   {'annotations': [{'bert_tokens': ['Ni', 'perna', 'adil']}],
                    'base_span': ((11, 22),)},
                   {'annotations': [{'bert_tokens': ['Rakvere']}],
                    'base_span': ((23, 30),)},
                   {'annotations': [{'bert_tokens': ['linna', 'metsa']}],
                    'base_span': ((31, 41),)},
                   {'annotations': [{'bert_tokens': ['siseneda']}],
                    'base_span': ((42, 50),)},
                   {'annotations': [{'bert_tokens': ['.']}], 'base_span': ((50, 51),)},
                   {'annotations': [{'bert_tokens': ['My', 'an', 'mari']}],
                    'base_span': ((52, 60),)},
                   {'annotations': [{'bert_tokens': ['mässu', 'lised']}],
                    'base_span': ((61, 71),)},
                   {'annotations': [{'bert_tokens': ['olid']}],
                    'base_span': ((72, 76),)},
                   {'annotations': [{'bert_tokens': ['ka']}], 'base_span': ((77, 79),)},
                   {'annotations': [{'bert_tokens': ['kuulnud']}],
                    'base_span': ((80, 87),)},
                   {'annotations': [{'bert_tokens': ['Bangko', 'kis']}],
                    'base_span': ((88, 97),)},
                   {'annotations': [{'bert_tokens': ['toimuvast']}],
                    'base_span': ((98, 107),)},
                   {'annotations': [{'bert_tokens': ['.']}],
                    'base_span': ((107, 108),)},
                   {'annotations': [{'bert_tokens': ['See']}],
                    'base_span': ((109, 112),)},
                   {'annotations': [{'bert_tokens': ['sundis']}],
                    'base_span': ((113, 119),)},
                   {'annotations': [{'bert_tokens': ['Mac', 'A', 'rt', 'hur', 'it']}],
                    'base_span': ((120, 131),)},
                   {'annotations': [{'bert_tokens': ['lahkuma']}],
                    'base_span': ((132, 139),)},
                   {'annotations': [{'bert_tokens': ['Ken', 'ya', 'st']}],
                    'base_span': ((140, 147),)},
                   {'annotations': [{'bert_tokens': ['ja']}],
                    'base_span': ((148, 150),)},
                   {'annotations': [{'bert_tokens': ['siirdu', 'ma']}],
                    'base_span': ((151, 159),)},
                   {'annotations': [{'bert_tokens': ['Me', 'ck', 'len', 'burgi']}],
                    'base_span': ((160, 172),)},
                   {'annotations': [{'bert_tokens': ['.']}],
                    'base_span': ((172, 173),)}]}
    
    assert layer_to_dict(text['bert_tokens_of_words']) == expected_output_layer

    # Case 2: bert tokens layer is an enveloping layer
    enveloping_bert_tokens_layer = \
        {'ambiguous': False,
         'attributes': ('nertag',),
         'enveloping': 'bert_tokens',
         'meta': {},
         'name': 'bert_tokens_2',
         'parent': None,
         'secondary_attributes': (),
         'serialisation_module': None,
         'spans': [{'annotations': [{'nertag': 'ORG'}], 'base_span': ((0, 1), (1, 2), (2, 3))},
                   {'annotations': [{'nertag': 'PER'}], 'base_span': ((11, 13),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((23, 30),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((52, 54),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((88, 94), (94, 97))},
                   {'annotations': [{'nertag': 'PER'}], 'base_span': ((120, 123), (123, 124))},
                   {'annotations': [{'nertag': 'PER'}], 'base_span': ((126, 129),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((140, 143),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((143, 145),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((160, 162),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((162, 164),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((164, 167),)},
                   {'annotations': [{'nertag': 'LOC'}], 'base_span': ((167, 172),)}]}
    text.add_layer( dict_to_layer(enveloping_bert_tokens_layer) )

    def rewriter_decorator(text_obj, sharing_words, shared_bert_tokens):
        return {'words':text_obj.text[sharing_words[0].start:sharing_words[-1].end], 
                'bert_tokens': [t.text for t in shared_bert_tokens],
                'nertags': [t.annotations[0]['nertag'] for t in shared_bert_tokens]}
    
    rewriter2 = BertTokens2WordsRewriter('bert_tokens_2', 
                                        input_words_layer = 'words', 
                                        output_attributes = ('words', 'bert_tokens', 'nertags'), 
                                        output_layer = 'bert_tokens_2_of_words',
                                        decorator = rewriter_decorator)
    rewriter2.tag(text)
    #from pprint import pprint
    #pprint( layer_to_dict(text['bert_tokens_2_of_words']) )
    
    expected_output_layer_2 = \
        {'ambiguous': False,
         'attributes': ('words', 'bert_tokens', 'nertags'),
         'enveloping': 'words',
         'meta': {},
         'name': 'bert_tokens_2_of_words',
         'parent': None,
         'secondary_attributes': (),
         'serialisation_module': None,
         'spans': [{'annotations': [{'bert_tokens': [['R', 'M', 'K']],
                                     'nertags': ['ORG'],
                                     'words': 'RMK'}],
                    'base_span': ((0, 3),)},
                   {'annotations': [{'bert_tokens': [['Ni']],
                                     'nertags': ['PER'],
                                     'words': 'Nipernaadil'}],
                    'base_span': ((11, 22),)},
                   {'annotations': [{'bert_tokens': [['Rakvere']],
                                     'nertags': ['LOC'],
                                     'words': 'Rakvere'}],
                    'base_span': ((23, 30),)},
                   {'annotations': [{'bert_tokens': [['My']],
                                     'nertags': ['LOC'],
                                     'words': 'Myanmari'}],
                    'base_span': ((52, 60),)},
                   {'annotations': [{'bert_tokens': [['Bangko', 'kis']],
                                     'nertags': ['LOC'],
                                     'words': 'Bangkokis'}],
                    'base_span': ((88, 97),)},
                   {'annotations': [{'bert_tokens': [['Mac', 'A'], ['hur']],
                                     'nertags': ['PER', 'PER'],
                                     'words': 'MacArthurit'}],
                    'base_span': ((120, 131),)},
                   {'annotations': [{'bert_tokens': [['Ken'], ['ya']],
                                     'nertags': ['LOC', 'LOC'],
                                     'words': 'Kenyast'}],
                    'base_span': ((140, 147),)},
                   {'annotations': [{'bert_tokens': [['Me'],
                                                     ['ck'],
                                                     ['len'],
                                                     ['burgi']],
                                     'nertags': ['LOC', 'LOC', 'LOC', 'LOC'],
                                     'words': 'Mecklenburgi'}],
                    'base_span': ((160, 172),)}]}
            
    assert layer_to_dict(text['bert_tokens_2_of_words']) == expected_output_layer_2
    
