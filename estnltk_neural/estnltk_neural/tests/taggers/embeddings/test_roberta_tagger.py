import pkgutil
import pytest
from estnltk import Text
import os


def check_if_transformers_is_available():
    return pkgutil.find_loader("transformers") is not None

def check_if_pytorch_is_available():
    return pkgutil.find_loader("torch") is not None

ESTROBERTA_PATH = os.environ.get('ESTROBERTA_PATH', None)


@pytest.mark.skipif(not check_if_transformers_is_available(),
                    reason="package tranformers is required for this test")
@pytest.mark.skipif(not check_if_pytorch_is_available(),
                    reason="package pytorch is required for this test")
@pytest.mark.skipif(ESTROBERTA_PATH is None,
                    reason="RobertaTagger's model location not known. "+\
                           "Set environment variable ESTROBERTA_PATH to model's local directory.")
def test_roberta_tagger_out_of_the_box():
    # Test that RobertaTagger works "out_of_the_box" if model is available
    from estnltk_neural.taggers.embeddings.bert.roberta_tagger import RobertaTagger
    roberta_tagger = RobertaTagger(bert_location=ESTROBERTA_PATH)
    text = Text(
        'Ilus suur karvane kass nurrus punasel diivanil. Ta on ise tee esimesel poolel. Valge j√§nes jooksis metsa!')
    text.tag_layer('sentences')
    roberta_tagger.tag(text)
    assert 'roberta_embeddings' in text.layers
    assert not text.roberta_embeddings.ambiguous

    for embedding_span in text.roberta_embeddings:
        assert len(embedding_span.bert_embedding) == 3072  # 768 * 4 

    assert len(text.words) < len(text.roberta_embeddings)


@pytest.mark.skipif(not check_if_transformers_is_available(),
                    reason="package tranformers is required for this test")
@pytest.mark.skipif(not check_if_pytorch_is_available(),
                    reason="package pytorch is required for this test")
@pytest.mark.skipif(ESTROBERTA_PATH is None,
                    reason="RobertaTagger's model location not known. "+\
                           "Set environment variable ESTROBERTA_PATH to model's local directory.")
def test_roberta_tagger_word_level_smoke():
    # Test that RobertaTagger works on word level
    from estnltk_neural.taggers.embeddings.bert.roberta_tagger import RobertaTagger
    roberta_tagger = RobertaTagger(bert_location=ESTROBERTA_PATH, token_level=False)
    text = Text(
        'Ilus suur karvane kass nurrus punasel diivanil. Ta on ise tee esimesel poolel. Valge j√§nes jooksis metsa, √ºtles KabernaakelHiks.')
    text.tag_layer('sentences')
    roberta_tagger.tag(text)
    assert 'roberta_embeddings' in text.layers
    assert not text.roberta_embeddings.ambiguous

    for embedding_span in text.roberta_embeddings:
        assert len(embedding_span.bert_embedding) == 3072  # 768 * 4 

    assert text.roberta_embeddings.text == text.words.text

# Fetches pairs (word, bert_tokens) from RobertaTagger's output
def _get_bert_tokens(text_obj, bert_layer='roberta_word_embeddings'):
    results = []
    for bert_span in text_obj[bert_layer]:
        results.append( (bert_span.text, list(bert_span.token)))
    return results

@pytest.mark.skipif(not check_if_transformers_is_available(),
                    reason="package tranformers is required for this test")
@pytest.mark.skipif(not check_if_pytorch_is_available(),
                    reason="package pytorch is required for this test")
@pytest.mark.skipif(ESTROBERTA_PATH is None,
                    reason="RobertaTagger's model location not known. "+\
                           "Set environment variable ESTROBERTA_PATH to model's local directory.")
def test_roberta_tagger_tokens_and_word_span_misaligment_bugfix():
    # 1) Test RobertaTagger for handling misalignment of word spans and embedding tokens
    from estnltk_neural.taggers.embeddings.bert.roberta_tagger import RobertaTagger
    text = Text('Ta s√§utsus: üòÉüíÅ?üíÅ!üíÅüíÅ? Mina vastu: ‚òé‚òè??? Tema seepeale: ‚ï≥üî•!üî•!')

    # token level
    roberta_tagger_1 = RobertaTagger(bert_location=ESTROBERTA_PATH, token_level=True)
    text.tag_layer('sentences')
    roberta_tagger_1.tag(text)
    assert 'roberta_embeddings' in text.layers
    for embedding_span in text.roberta_embeddings:
        assert len(embedding_span.bert_embedding) == 3072  # 768 * 4 

    # word level
    roberta_tagger_2 = RobertaTagger(output_layer='roberta_word_embeddings',
                                     bert_location=ESTROBERTA_PATH, token_level=False)
    roberta_tagger_2.tag(text)
    assert 'roberta_word_embeddings' in text.layers
    for embedding_span in text.roberta_word_embeddings:
        assert len(embedding_span.bert_embedding) == 3072  # 768 * 4 
    assert text.roberta_word_embeddings.text == text.words.text
    
    # 2) Test RobertaTagger for handling ‚Ä¶ and ... replacements
    text = Text('Ta s√§utsus: üòÉüíÅ?üíÅ!üíÅüíÅ? Mina vastu: ‚òé‚Ä¶??? ...! Tema seepeale: ‚ï≥üî•!üî•!')
    text.tag_layer('sentences')
    roberta_tagger_2.tag(text)
    assert 'roberta_word_embeddings' in text.layers
    for embedding_span in text.roberta_word_embeddings:
        assert len(embedding_span.bert_embedding) == 3072  # 768 * 4 
    assert text.roberta_word_embeddings.text == text.words.text
    
    # 3) Test RobertaTagger for handling tokens where starting ‚ñÅ is separated from
    #    the first token of the word, such as in 'iaido' -> ['‚ñÅ', 'ia', 'ido']
    text = Text('Sobudo on t√§ielik v√µitluskunstide s√ºsteem , mis koosneb erinevatest aladest sh : '+\
                'jodo , aikido , iaido ja jukempo . Tema seepeale: ‚ï≥üî•!üî•!')
    text.tag_layer('sentences')
    roberta_tagger_2.tag(text)
    assert 'roberta_word_embeddings' in text.layers
    for embedding_span in text.roberta_word_embeddings:
        assert len(embedding_span.bert_embedding) == 3072  # 768 * 4 
    assert text.roberta_word_embeddings.text == text.words.text

    # 4) Test handling inputs that have '\xad' and/or '‚Ä¶' symbol.
    partial_overlap1 = Text('Yangi¬≠Millsi kalibratsiooniv√§lja kvantteooria ‚Ä¶?').tag_layer('sentences')
    roberta_tagger_2.tag(partial_overlap1)
    words_and_bert_tokens = _get_bert_tokens(partial_overlap1, bert_layer=roberta_tagger_2.output_layer)
    assert words_and_bert_tokens == \
        [('Yangi', ['‚ñÅY', 'angi']), 
         ('\xad', ['\xad']), 
         ('Millsi', ['M', 'ill', 'si']), 
         ('kalibratsiooniv√§lja', ['‚ñÅkal', 'ib', 'ratsiooni', 'v√§lja']), 
         ('kvantteooria', ['‚ñÅkvant', 'teooria']), 
         ('‚Ä¶?', ['‚ñÅ...', '?'])]
    partial_overlap2 = Text('Ning K√§lleni¬≠Lehmanni teoreem').tag_layer('sentences')
    roberta_tagger_2.tag(partial_overlap2)
    words_and_bert_tokens = _get_bert_tokens(partial_overlap2, bert_layer=roberta_tagger_2.output_layer)
    assert words_and_bert_tokens == \
        [('Ning', ['‚ñÅNing']), 
         ('K√§lleni', ['‚ñÅK√§', 'lle', 'ni']), 
         ('\xad', ['\xad']), 
         ('Lehmanni', ['Le', 'hma', 'nni']), 
         ('teoreem', ['‚ñÅte', 'or', 'eem'])]

