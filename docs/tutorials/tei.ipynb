{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Estonian Koondkorpus\n",
    "=================================\n",
    "\n",
    "### Converting TEI files to Estnltk's JSON format\n",
    "\n",
    "This tutorial describes how to work with [Eesti\n",
    "Koondkorpus](http://www.cl.ut.ee/korpused/segakorpus/) and import the\n",
    "files in TEI format to JSON format used by Estnltk. After this\n",
    "conversion, you can check (database\\_tutorial) to see how you could\n",
    "import all these documents to a fast searchable text database. First,\n",
    "dowload all the XML TEI format files to your computer, into a folder\n",
    "`corpora/koond`. Check the subcategories of the site to find the\n",
    "download links.\n",
    "\n",
    "On my computer, I have the following list of files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    ls -1 corpora/koond/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    Agraarteadus.zip\n",
    "    Arvutitehnika.zip\n",
    "    Doktoritood.zip\n",
    "    EestiArst.zip\n",
    "    Ekspress.zip\n",
    "    foorum_uudisgrupp_kommentaar.zip\n",
    "    Horisont.zip\n",
    "    Ilukirjandus.zip\n",
    "    Kroonika.zip\n",
    "    LaaneElu.zip\n",
    "    Luup.zip\n",
    "    Maaleht.zip\n",
    "    Paevaleht.zip\n",
    "    Postimees.zip\n",
    "    Riigikogu.zip\n",
    "    Seadused.zip\n",
    "    SLOleht.tar.gz\n",
    "    Teadusartiklid.zip\n",
    "    Valgamaalane.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we go into this directory and unzip all the files.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    cd corpora/koond/\n",
    "    unzip \"*.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we have a bunch of folders with structure similar below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "├── Kroonika\n",
    "│   ├── bin\n",
    "│   │   ├── koondkorpus_main_header.xml\n",
    "│   │   └── tei_corpus.rng\n",
    "│   └── Kroon\n",
    "│       ├── bin\n",
    "│       │   └── header_aja_kroonika.xml\n",
    "│       └── kroonika\n",
    "│           ├── kroonika_2000\n",
    "│           │   ├── aja_kr_2000_12_08.xml\n",
    "│           │   ├── aja_kr_2000_12_15.xml\n",
    "│           │   ├── aja_kr_2000_12_22.xml\n",
    "│           │   └── aja_kr_2000_12_29.xml\n",
    "│           ├── kroonika_2001\n",
    "│           │   ├── aja_kr_2001_01_05.xml\n",
    "│           │   ├── aja_kr_2001_01_12.xml\n",
    "│           │   ├── aja_kr_2001_01_19.xml\n",
    "│           │   ├── aja_kr_2001_01_22.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders `bin` contain headers and corpus descriptions and can go\n",
    "hiearchially down the way. If we are only interested in the actual\n",
    "articles themselves, we should ignore all files that contain `bin` in\n",
    "their path and only use files that end with `.xml`.\n",
    "\n",
    "Anyway, here is a script that tries its best at doing some basic\n",
    "conversion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from estnltk.teicorpus import parse_tei_corpus\n",
    "from estnltk.corpus import write_document\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger('koondkonverter')\n",
    "\n",
    "\n",
    "def get_target(fnm):\n",
    "    if 'drtood' in fnm:\n",
    "        return 'dissertatsioon'\n",
    "    if 'ilukirjandus' in fnm:\n",
    "        return 'tervikteos'\n",
    "    if 'seadused' in fnm:\n",
    "        return 'seadus'\n",
    "    if 'EestiArst' in fnm:\n",
    "        return 'ajakirjanumber'\n",
    "    if 'foorum' in fnm:\n",
    "        return 'teema'\n",
    "    if 'kommentaarid' in fnm:\n",
    "        return 'kommentaarid'\n",
    "    if 'uudisgrupid' in fnm:\n",
    "        return 'uudisgrupi_salvestus'\n",
    "    if 'jututoad' in fnm:\n",
    "        return 'jututoavestlus'\n",
    "    if 'stenogrammid' in fnm:\n",
    "        return 'stenogramm'\n",
    "    return 'artikkel'\n",
    "\n",
    "\n",
    "def process(start_dir, out_dir, encoding=None):\n",
    "    for dirpath, dirnames, filenames in os.walk(start_dir):\n",
    "        if len(dirnames) > 0 or len(filenames) == 0 or 'bin' in dirpath:\n",
    "            continue\n",
    "        for fnm in filenames:\n",
    "            full_fnm = os.path.join(dirpath, fnm)\n",
    "            out_prefix = os.path.join(out_dir, fnm)\n",
    "            target = get_target(full_fnm)\n",
    "            if os.path.exists(out_prefix + '_0.txt'):\n",
    "                logger.info('Skipping file {0}, because it seems to be already processed'.format(full_fnm))\n",
    "                continue\n",
    "            logger.info('Processing file {0} with target {1}'.format(full_fnm, target))\n",
    "            docs = parse_tei_corpus(full_fnm, target=target, encoding=encoding)\n",
    "            for doc_id, doc in enumerate(docs):\n",
    "                out_fnm = '{0}_{1}.txt'.format(out_prefix, doc_id)\n",
    "                logger.info('Writing document {0}'.format(out_fnm))\n",
    "                write_document(doc, out_fnm)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Convert a bunch of TEI XML files to Estnltk JSON files\")\n",
    "    parser.add_argument('startdir', type=str, help='The path of the downloaded and extracted koondkorpus files')\n",
    "    parser.add_argument('outdir', type=str, help='The directory to store output results')\n",
    "    parser.add_argument('-e', '--encoding', type=str, default=None, help='Encoding of the TEI XML files')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    process(args.startdir, args.outdir, args.encoding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an output directory `corpora/converted` for the results and run\n",
    "the scripts with appropriate parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    python3 -m estnltk.examples.convert_koondkorpus corpora/koond corpora/converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be downloaded from here:\n",
    "<http://ats.cs.ut.ee/keeletehnoloogia/estnltk/koond.zip> .\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> Currently, this zip package does not include files from\n",
    "> `SLOleht.tar.gz`. In order to include the files from SLOleht, please\n",
    "> download the `SLOleht.tar.gz`, unpack the contents, and use the script\n",
    "> `estnltk.examples.convert_koondkorpus` to obtain the missing part of\n",
    "> the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentence tokenizer for koondkorpus\n",
    "\n",
    "The default sentence tokenizer can produce a tokenization with suboptimal quality when applied to the `koondkorpus` files, as the texts in the `koondkorpus` have already been tokenized at the word level (and this is an unexpected input to the default sentence tokenizer). \n",
    "However, EstNLTK also provides a special sentence tokenizer, **SentenceTokenizerForKoond**, which provides fixes to several known sentence-splitting problems in the corpus.\n",
    "\n",
    "In the following examples, the default sentence tokenizer is compared with the **SentenceTokenizerForKoond** in processing a text with problematic tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a text containing problematic tokenisations (as can be found in koondkorpus):\n",
    "problematic_tok = '''Kõigi võistlejate seast jõudsid punktikohale Tipp ( 2. ) ja Täpp ( 4. ) ja Käpp ( 7. ) .\n",
    "Bänd , mis moodustati 1968 . aastal .\n",
    "Kirjandusel ( resp. raamatul ) on läbi aegade olnud erinevaid funktsioone .\n",
    "Iga päev teeme valikuid.Valime kõike alates pesupulbrist ja lõpetades autopesulatega.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kõigi võistlejate seast jõudsid punktikohale Tipp ( 2. )',\n",
       " 'ja Täpp ( 4. )',\n",
       " 'ja Käpp ( 7. )',\n",
       " '.',\n",
       " 'Bänd , mis moodustati 1968 .',\n",
       " 'aastal .',\n",
       " 'Kirjandusel ( resp.',\n",
       " 'raamatul ) on läbi aegade olnud erinevaid funktsioone .',\n",
       " 'Iga päev teeme valikuid.Valime kõike alates pesupulbrist ja lõpetades autopesulatega.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from estnltk import Text\n",
    "# Use the default sentence tokenizer\n",
    "text = Text( problematic_tok )\n",
    "text.sentence_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kõigi võistlejate seast jõudsid punktikohale Tipp ( 2. ) ja Täpp ( 4. ) ja Käpp ( 7. ) .',\n",
       " 'Bänd , mis moodustati 1968 . aastal .',\n",
       " 'Kirjandusel ( resp. raamatul ) on läbi aegade olnud erinevaid funktsioone .',\n",
       " 'Iga päev teeme valikuid.',\n",
       " 'Valime kõike alates pesupulbrist ja lõpetades autopesulatega.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from estnltk.tokenizers.sent_tokenizer_for_koond import SentenceTokenizerForKoond\n",
    "from estnltk import Text\n",
    "kwargs = {\n",
    "    \"sentence_tokenizer\": SentenceTokenizerForKoond()\n",
    "}\n",
    "# Use the koondkorpus specific tokenizer\n",
    "text = Text( problematic_tok, **kwargs )\n",
    "text.sentence_texts"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
