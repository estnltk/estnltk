<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Text classification tool &mdash; estnltk 1.3 documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="estnltk 1.3 documentation" href="../index.html" />
    <link rel="next" title="estnltk.clausesegmenter module" href="../api/clausesegmenter.html" />
    <link rel="prev" title="Tables" href="morf_tables.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../api/clausesegmenter.html" title="estnltk.clausesegmenter module"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="morf_tables.html" title="Tables"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">estnltk 1.3 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="text-classification-tool">
<h1>Text classification tool<a class="headerlink" href="#text-classification-tool" title="Permalink to this headline">¶</a></h1>
<p>Estnltk classifier is <em>machine learning</em> software for organizing data into categories.
It is a separate tool from estnltk, although it depends on it.
See the repository <a class="reference external" href="https://github.com/estnltk/textclassifier">https://github.com/estnltk/textclassifier</a> .</p>
<p>To install it from the PyPi repository, type:</p>
<div class="highlight-python"><div class="highlight"><pre>pip install estnltk-textclassifier
</pre></div>
</div>
<p>And run unit tests:</p>
<div class="highlight-python"><div class="highlight"><pre>python -m unittest discover textclassifier
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre>Ran 40 tests in 16.088s

OK
</pre></div>
</div>
<div class="section" id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s start right away with an example to show what kind of problems the software is designed to solve.
Consider you are given a dataset of user reviews and have to classify them either as positive, negative or neutral (See <tt class="docutils literal"><span class="pre">data/hinnavaatlus.csv</span></tt> for full dataset):</p>
<table border="1" class="docutils">
<colgroup>
<col width="4%" />
<col width="93%" />
<col width="3%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Kommentaari ID</th>
<th class="head">Kommentaar</th>
<th class="head">Meelsus</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>4</td>
<td>Väike, aga tubli firma!</td>
<td>Neutraalne</td>
</tr>
<tr class="row-odd"><td>8</td>
<td>väga hea firma</td>
<td>Positiivne</td>
</tr>
<tr class="row-even"><td>10</td>
<td>Viimasel ajal pole midagi halba öelda, aga samas ei konkureeri nad kuidagi Genneti, Ordiga ei hindadelt ega teeninduselt. Toorikute ja tindi ostmiseks samas hea koht ja kuna müüjaid on rohkem valima hakatud, siis võiks 2 ikka ära panna - tuleks 3 kui hinadele ei pandaks kirvest ja toodete saadavus oleks parem.</td>
<td>Negatiivne</td>
</tr>
<tr class="row-odd"><td>11</td>
<td>Fotode kvaliteet väga pro ja &#8220;jjk&#8221; seal töötamise ajal leiti ikka paljudele asjadele väga meeldivad lahendused. Samas hilisem läpaka ost sujus ka väga meeldivalt - sain esialgse rahas ostusoovi vormistada ümber järelmaksule...äärmiselt asjalik teenindus.</td>
<td>Positiivne</td>
</tr>
<tr class="row-even"><td>13</td>
<td>Ainult positiivsed kogemused</td>
<td>Positiivne</td>
</tr>
<tr class="row-odd"><td>16</td>
<td>Viimane kord, kui käisin suutis leti taga askeldav ~60 aastane mees tegutseda nii aeglaselt, et minu seal veedetud 10min jooksul pani juba vähemalt 6-7 klienti putku. Garantiiga ka kurvad kogemused, neid poleks, saaks isegi kahe vast. Vihaseks ajab lihtsalt vahest nende teenindus!</td>
<td>Negatiivne</td>
</tr>
<tr class="row-even"><td>17</td>
<td>Väga head hinnad!!!</td>
<td>Positiivne</td>
</tr>
</tbody>
</table>
<p><em>NB! All file I/O of the software assumes the UTF-8 encoding.</em></p>
<div class="section" id="step-1-defining-the-classification-task">
<h3>Step 1. Defining the classification task<a class="headerlink" href="#step-1-defining-the-classification-task" title="Permalink to this headline">¶</a></h3>
<p>One needs to tell the software, what columns contain features and what columns contain the results.
This can be done by writing a definitions file (let&#8217;s call it <tt class="docutils literal"><span class="pre">hinnavaatlus.def</span></tt>), which in our example case would look like:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">Kommentaar</span>
<span class="p">[</span><span class="n">label</span><span class="p">]</span>
<span class="n">Meelsus</span>
<span class="p">[</span><span class="n">confidence</span><span class="p">]</span>
<span class="n">Kindlus</span>
</pre></div>
</div>
<ul class="simple">
<li>The <tt class="docutils literal"><span class="pre">[features]</span></tt> section contains one feature column name per row.
Our example only has one feature column.
We exclude the <em>comment id</em> column as it does not contain any information about the user opinion.</li>
<li>The <tt class="docutils literal"><span class="pre">[label]</span></tt> section contains the name of the column, where the category label is.</li>
<li>The <tt class="docutils literal"><span class="pre">[confidence]</span></tt> is the name of the column, where the classification confidence is stored (this will be discussed later).</li>
</ul>
</div>
<div class="section" id="step-2-building-the-classification-model">
<h3>Step 2. Building the classification model<a class="headerlink" href="#step-2-building-the-classification-model" title="Permalink to this headline">¶</a></h3>
<p>In this example, we use the command line training program to build a model for Hinnavaatlus.ee user review classification.
We can see the possible parameters by issuing command:</p>
<div class="highlight-python"><div class="highlight"><pre>python3 -m textclassifier.train -h
</pre></div>
</div>
<p>That will output:</p>
<div class="highlight-python"><div class="highlight"><pre>usage: textclassifier.train [-h] [--synonyms SYNONYMS] [-r REPORT]
                            [--sheet SHEET] [--sep SEP]
                            settings dataset model

positional arguments:
settings              Settings definitions containing features columns,
                        label column and confidence column.
dataset               Dataset to use for training. Must contain columns
                        defined in settings file. It is possible to load .csv
                        and .xlsx files.
model                 The path to store the trained model.

optional arguments:
-h, --help            show this help message and exit
--synonyms SYNONYMS   File containing a set of technical synonyms, one set
                        per line.
-r REPORT, --report REPORT
                        The name of the report. The report is written as two
                        files [name].html and [name]_misclassified_data.html
--sheet SHEET         Sheet name if loading data from Excel file (default
                        read the first sheet).
--sep SEP             Column separator for CSV files (default is ,).
</pre></div>
</div>
<p>There are two parameters we need to discuss separately.</p>
<p>First one is the <tt class="docutils literal"><span class="pre">--synonyms</span></tt> parameter, which can be used to fine-tune the classifyer by specifiyng a list of technical synonyms.
The argument should contain the path to a file, where each line defines a list of synonyms. For our example, we could define a file like:</p>
<div class="highlight-python"><div class="highlight"><pre>firma ettevõte kauplus
sülearvuti läpakas rüperaal laptop
</pre></div>
</div>
<p>This file is optional, but can be used to enhance the classification accuracy.</p>
<p>The second parameter is <tt class="docutils literal"><span class="pre">--report</span></tt> that does additional computation during training and creates two HTML files containing detailed performance characterstics of the trained model (this will be discussed more indepth later).
The report can be used to improve the categories of the data, to increase the technical synonym list etc.</p>
<p>However, we currently ignore this extra functionality and just train the classifier by issuing command:</p>
<div class="highlight-python"><div class="highlight"><pre>python3 -m textclassifier.train definitions/hinnavaatlus.def data/hinnavaatlus.csv models/hinnavaatlus.bin
</pre></div>
</div>
<p>Often the log outputs warnings related to <tt class="docutils literal"><span class="pre">numpy</span></tt>, <tt class="docutils literal"><span class="pre">sckikit-learn</span></tt> and other dependencies, but these can be ingored until no specific errors are generated.
The reason is that the dependencies are constantly being developed and upgraded and most warnings are related to their development:</p>
<div class="highlight-python"><div class="highlight"><pre>INFO:train:Loading settings from definitions/hinnavaatlus.def and techsynonyms from None .
Namespace(dataset=&#39;data/hinnavaatlus.csv&#39;, model=&#39;models/hinnavaatlus.bin&#39;, report=None, sep=&#39;,&#39;, settings=&#39;definitions/hinnavaatlus.def&#39;, sheet=0, synonyms=None)
INFO:root:Reading dataset data/hinnavaatlus.csv
INFO:clf:Training new model with settings{&#39;unifier&#39;: &lt;Estnltk.synunifier.SynUnifier object at 0x7f830a450f60&gt;, &#39;label&#39;: &#39;Meelsus&#39;, &#39;confidence&#39;: &#39;Kindlus&#39;, &#39;features&#39;: [&#39;Kommentaar&#39;]} and dataframe with 813 rows
DEBUG:clf:Fitting classifier with 456 features and 813 examples and 3 disctinctive labels
INFO:clf:Skipping report generation.
INFO:clf:Training finished. Took total of 2.2 seconds.
INFO:root:Saving classifier to models/hinnavaatlus.bin
INFO:train:Done!
</pre></div>
</div>
<p>The log tells as that the trained model uses a combination of 456 word phrases for classification and that the full dataset contained 813 user reviews.
The saved model is stored in <tt class="docutils literal"><span class="pre">models/hinnavaatlus.bin</span></tt> file.</p>
<dl class="docutils">
<dt>..note:: The classifier is capable of working with both CSV and XLSX files, but make sure CSV files use character &#8221; for quoting.</dt>
<dd>With Excel XLSX, we have had problems loading the dataset, if Excel contains some extra functionality.
For example, the software cannot load XLSX files with Data Autofilters.</dd>
</dl>
</div>
<div class="section" id="step-3-using-the-model-to-categorize-data">
<h3>Step 3. Using the model to categorize data<a class="headerlink" href="#step-3-using-the-model-to-categorize-data" title="Permalink to this headline">¶</a></h3>
<p>The classification command line program accepts following arguments:</p>
<div class="highlight-python"><div class="highlight"><pre>$ python3 -m textclassifier.classify -h
usage: textclassifier.classify [-h] [--insheet INSHEET] [--insep INSEP]
                         [--outsheet OUTSHEET] [--outsep OUTSEP]
                         indata outdata model

positional arguments:
  indata               Path for the input dataset that will be classified. It
                       is possible to load .csv and .xlsx files.
  outdata              Path where the classified dataset will be stored. It is
                       possible to save .csv and .xlsx files
  model                The path of the classification model.

optional arguments:
  -h, --help           show this help message and exit
  --insheet INSHEET    Sheet name if reading data from Excel file (default is
                       the first sheet).
  --insep INSEP        Column separator for reading CSV files (default is ,).
  --outsheet OUTSHEET  Sheet name if saving as an Excel file (default is
                       Sheet1).
  --outsep OUTSEP      Column separator for saving CSV files (default is ,).
</pre></div>
</div>
<p>The data we want to categorize has to have all columns that we defined in <tt class="docutils literal"><span class="pre">[features]</span></tt> section in classification task definitions file <tt class="docutils literal"><span class="pre">hinnavaatlus.def</span></tt> that we used during training/model building step.
The software will fill out the <tt class="docutils literal"><span class="pre">[label]</span></tt> and <tt class="docutils literal"><span class="pre">[confidence]</span></tt> columns itself.
The datafiles can contain other columns such as IDS, dates etc that are not used by the classifier, but stored in output as well.</p>
<p>Consider these four example reviews I have written (<tt class="docutils literal"><span class="pre">data/hinnavaatlus_test.csv</span></tt>). We expect the first to be positive, second negative, third positive and last one also negative.</p>
<table border="1" class="docutils">
<colgroup>
<col width="100%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Kommentaar</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Ettevõttega ainult positiivsed kogemused</td>
</tr>
<tr class="row-odd"><td>Sain firmaga petta, tellisin toote internetist, aga toodet ei tulnud, samuti ei ole saanud tagasi raha.</td>
</tr>
<tr class="row-even"><td>Kõik toimis nii nagu lubatud: arvuti tuli kohale õigeaegselt ja kõik toimis.</td>
</tr>
<tr class="row-odd"><td>Muidu ok, aga toode läks pärast nädalast kasutamist rikki. Garantiiremont on aega võtnud juba üle kuu!!</td>
</tr>
</tbody>
</table>
<p>Let&#8217;s classify the dataset:</p>
<div class="highlight-python"><div class="highlight"><pre>$ python3 -m textclassifier.classify data/hinnavaatlus_test.csv result.csv models/hinnavaatlus.bin
INFO:root:Reading dataset data/hinnavaatlus_test.csv
INFO:root:Loading classifier from models/hinnavaatlus.bin
INFO:classify:Performing classification on 4 examples.
INFO:clf:Starting classification task.
INFO:clf:Classification completed. Took total of 0.0 seconds.
INFO:root:Writing dataset result.csv
INFO:classify:Done!
</pre></div>
</div>
<p>We save the results into file <tt class="docutils literal"><span class="pre">results.csv</span></tt>, which contains the following:</p>
<table border="1" class="docutils">
<colgroup>
<col width="81%" />
<col width="9%" />
<col width="10%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Kommentaar</th>
<th class="head">Meelsus</th>
<th class="head">Kindlus</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Ettevõttega ainult positiivsed kogemused</td>
<td>Positiivne</td>
<td>0.7380716036</td>
</tr>
<tr class="row-odd"><td>Sain firmaga petta, tellisin toote internetist, aga toodet ei tulnud, samuti ei ole saanud tagasi raha.</td>
<td>Negatiivne</td>
<td>0.4742324816</td>
</tr>
<tr class="row-even"><td>Kõik toimis nii nagu lubatud: arvuti tuli kohale õigeaegselt ja kõik toimis.</td>
<td>Positiivne</td>
<td>0.7829001265</td>
</tr>
<tr class="row-odd"><td>Toode läks pärast nädalast kasutamist rikki. Garantiiremont on aega võtnud juba üle kuu!!</td>
<td>Negatiivne</td>
<td>0.8060670574</td>
</tr>
</tbody>
</table>
<p>We see that instead of one column, there are now also label and confidence columns named <em>Meelsus</em> and <em>Kindlus</em> respectively.
Although we see that this time the classifier has correctly categorized all four examples, it is not always the case.
Classifier makes errors as it is not always possible automatically tell from the text, what is the correct category.
Confidence value and its usage is covered later when we discuss the automatically generated report.</p>
</div>
</div>
<div class="section" id="debugging-the-classifier">
<h2>Debugging the classifier<a class="headerlink" href="#debugging-the-classifier" title="Permalink to this headline">¶</a></h2>
<p>The classifier learns the properties it needs to classify data from the training data supplied at the model building step.
What it does in general, is that it computes the correlation between words and category labels and then uses this information to predict the most probable outcome.
As it uses statistical reasoning to make its decision, there will be errors. Few reasons why errors are made:</p>
<ol class="arabic simple">
<li>For a certain category label, there may be too few examples. This makes it hard to learn the properties of the category.</li>
<li>One or more categories may be semantically very similar, thus making it harder to make difference between them.</li>
<li>The dataset that the classifier was trained on, is too old or too different from the dataset that its used to predict categories.</li>
</ol>
<p>During the model building step, we can optionally generate report that gives detailed information about how many errors the classifier is expected to make and what categories are most troublesome.
Let&#8217;s use our hinnavaatlus.ee example dataset and build a model now with a generated report:</p>
<div class="highlight-python"><div class="highlight"><pre>$ python3 -m textclassifier.train definitions/hinnavaatlus.def data/hinnavaatlus.csv models/hinnavaatlus.bin --report hinnavaatlus
INFO:train:Loading settings from definitions/hinnavaatlus.def and techsynonyms from None .
...
INFO:root:Saving classifier to models/hinnavaatlus.bin
INFO:root:Writing HTML content to hinnavaatlus.html
INFO:root:Writing HTML content to hinnavaatlus_misclassified_data.html
INFO:train:Done!
</pre></div>
</div>
<p>For the purposes of the report, the classifier splits the dataset into ten pieces and performs ten train-test cycles to evaluate its precision.
This is called <em>stratified 10-fold cross-validation</em>.
This is required in order to get realistic estimates how the model will perform on unseen data.
Report can be generated only on training dataset as we need <em>true</em> categories for estimating the accuracy.
However, as the cross-validation uses random splits each time, there are smaller variations in the reported accuracy.
The resulting classifier will be built using full data, so the real accuracy should be always slightly better
than the number reported.</p>
<p>But let&#8217;s now analyze the contents of <tt class="docutils literal"><span class="pre">hinnavaatlus.html</span></tt> document.</p>
<div class="section" id="classification-report">
<h3>Classification report<a class="headerlink" href="#classification-report" title="Permalink to this headline">¶</a></h3>
<p>The first section of the <tt class="docutils literal"><span class="pre">hinnavaatlus.html</span></tt> file contains the classification report:</p>
<img alt="Classification report" src="../_images/classification_report.png" />
<p>It denotes the <em>precision</em>, <em>recall</em> and <em>F1-score</em> for each category as seen above.
The most important metric is the F1-score, the harmonic mean of precision and recall.
See <a class="reference external" href="http://en.wikipedia.org/wiki/Precision_and_recall">http://en.wikipedia.org/wiki/Precision_and_recall</a> for detailed descriptions of these metrics.</p>
<p>We see that the <em>positive</em> reviews are most easy to detect.
Partly, this is because there are about 500 such reviews, twice as much as <em>negative</em> and <em>neutral</em> combined.</p>
<p>But overall, we see that the F1-score is pretty low as about only 60% of predictions are correct.
Now, one tip on improving the overall accuracy is to minimize the number of different categories.
For example, if we would like to do market analysis on how the public opinion about various companies differs in time,
we can join the <em>neutral</em> reviews with <em>negative</em> ones.
Time series of number of <em>neutral</em> opinions would not be interesting to marketing staff anyway.
So, we can essentially trade <em>neutral</em> reviews for better overall accuracy.</p>
<p>Let&#8217;s see, if this tip makes a difference. File <tt class="docutils literal"><span class="pre">data/hinnavaatlus_simple.csv</span></tt> contains same training data as before, but now only with <em>positive</em> and <em>negative</em> classes.
After retraining, we see following numbers in classification report.</p>
<img alt="Classification report" src="../_images/classification_report_improved.png" />
<p>We see that this simple change increased the overall accuracy by 20 percent points, from 63.3% before to 80.3% now.
Thus, one simplest, but most effective way to increase the performance is to use only categories, which are required for a particular task.</p>
</div>
<div class="section" id="confidence-cutoff-vs-f1-curve">
<h3>Confidence cutoff vs F1 curve<a class="headerlink" href="#confidence-cutoff-vs-f1-curve" title="Permalink to this headline">¶</a></h3>
<p>As discussed earlier, the <tt class="docutils literal"><span class="pre">confidence</span></tt> column denotes how confident is the prediction.
Each classified data point has a confidence score &#8211; the higher the score, the lower the probability of making an error.
In other words, it describes how hard it is to classify the data point.
The plot shows how the overall accuracy changes by including only data points where the confidence is greater or equal to the confidence cutoff treshold.</p>
<a class="reference internal image-reference" href="../_images/confidence.png"><img alt="Confidence cutoff / F1 curve" src="../_images/confidence.png" style="width: 30em;" /></a>
<p>The red line depicts the 90% accuracy and green line 95% accuracy.</p>
<p>The second tip to improve overall accuracy of the predictions is to throw away examples that are harder to classify.
Confidence cutoff / F1 curve helps to determine the high enough confidence to obtain certain accuracy.</p>
<p>In this case, confidence &gt;= 80% will give us overall accuracy of 90%.
This is especially useful, if we have a lot of data and can throw some of it away for the sake of better accuracy.
However, be careful, as the confidence of the predictions depend on their true categories, some of them may be naturally harder to classify.
Thus, when filtering the results by their confidence, the proportions of classes in filtered result might change and get out of original proportion.</p>
<p>Whether this is a problem or not, depends much on for what kind of statistics the results are needed for.
For example, when we compare the change of ratio of positive and negative reviews for some company over four quarters of a year, we only require that the proportions of the predictions are consistent.
Thus, when we use same confidence cutoff for all quarters, there should not be any problems.</p>
<p>On the other hand, if we are interested in ratios, we could still use the full dataset, even if the overall accuracy is lower.
In case of computing ratios, some errors can cancel each other out.
For example, a negative review classified as positive and a positive one classified as negative cancel each other out.
In any way, these ratios need to interpreted with knowing that they are based on data, that is 80% accurate.</p>
</div>
<div class="section" id="coverage-vs-f1-curve">
<h3>Coverage vs F1 curve<a class="headerlink" href="#coverage-vs-f1-curve" title="Permalink to this headline">¶</a></h3>
<p>The coverage plot shows how the overall accuracy changes by removing data that is harder to classify.
Typically, by removing the harder examples, we obtain better overall accuracy.
This is complementary to confidence cutoff vs F1 curve described in previous section.</p>
<a class="reference internal image-reference" href="../_images/coverage.png"><img alt="Coverage / F1 curve" src="../_images/coverage.png" style="width: 30em;" /></a>
<p>We see that for obtaining 90% accuracy, we can keep only 55% of the data.</p>
</div>
<div class="section" id="significant-features-by-labels">
<h3>Significant features by labels<a class="headerlink" href="#significant-features-by-labels" title="Permalink to this headline">¶</a></h3>
<p>The next section displays 100 most significant features for each category.
Features written in black and red denote features that are respectively contributing towards and against assigning the particular class label.
Both are equally important, but they should be interpreted differently, when debugging the classifier.</p>
<a class="reference internal image-reference" href="../_images/significant_features.png"><img alt="Significant features by labels" src="../_images/significant_features.png" style="width: 60em;" /></a>
<p>Our current example uses only two categories, thus the important features are exactly the opposite of each other. In case of three or more categories,
there will be more variations.</p>
</div>
<div class="section" id="misclassified-examples">
<h3>Misclassified examples<a class="headerlink" href="#misclassified-examples" title="Permalink to this headline">¶</a></h3>
<p>File <tt class="docutils literal"><span class="pre">hinnavaatlus_misclassified_data.html</span></tt> contains a number of sections and lists all examples that were misclassified.
It displays the <em>true category label</em> and the <em>predicted category label</em>.</p>
<a class="reference internal image-reference" href="../_images/misclassified_data.png"><img alt="Misclassified data" src="../_images/misclassified_data.png" style="width: 60em;" /></a>
<p>The first review is an example, where word &#8220;positive&#8221; tricks the classifier to think that the review is actually positive.
The negating word &#8220;ei&#8221; is too far away so the computer fails to understand the semantics of the review.
We also see some reviews that are hard to classify strictly as positive or negative as they contain both positive and negative feedback.
The misclassified data also has black and red bold words that denote the important features.</p>
<p>By analyzing the significant features and misclassified examples, one can see which features could be aggregated and write them as technical synonyms.
This can make it easier for the classifier to make predictions.</p>
<p>For example, let&#8217;s create a file <tt class="docutils literal"><span class="pre">hinnavaatlus.txt</span></tt> containing some synonymous words:</p>
<div class="highlight-python"><div class="highlight"><pre>firma ettevõte kauplus pood
sülearvuti läpakas rüperaal laptop
suurepärane hea super superluks positiivne
halb ebameeldiv tüütu
</pre></div>
</div>
<p>The first word of each line denotes the main synonym and all other words on the line are replaced by the first word.
Let&#8217;s see, if this small change reflects in prediction accuracy:</p>
<div class="highlight-python"><div class="highlight"><pre>$ python3 -m estnltk.textclassifier.train definitions/hinnavaatlus.def data/hinnavaatlus_simple.csv models/hinnavaatlus.bin --report hinnavaatlus --synonyms hinnavaatlus.txt
</pre></div>
</div>
<img alt="Classification report technical synonyms" src="../_images/classification_report_tech.png" />
<p>Compared to previous 80.3%, we get a better result, although it is only 0.7 percent point higher score.
Also, note that due to cross-validation, different runs can give slightly different accuracy estimates.
But on average, the results of many runs tests should be improved.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Text classification tool</a><ul>
<li><a class="reference internal" href="#usage">Usage</a><ul>
<li><a class="reference internal" href="#step-1-defining-the-classification-task">Step 1. Defining the classification task</a></li>
<li><a class="reference internal" href="#step-2-building-the-classification-model">Step 2. Building the classification model</a></li>
<li><a class="reference internal" href="#step-3-using-the-model-to-categorize-data">Step 3. Using the model to categorize data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#debugging-the-classifier">Debugging the classifier</a><ul>
<li><a class="reference internal" href="#classification-report">Classification report</a></li>
<li><a class="reference internal" href="#confidence-cutoff-vs-f1-curve">Confidence cutoff vs F1 curve</a></li>
<li><a class="reference internal" href="#coverage-vs-f1-curve">Coverage vs F1 curve</a></li>
<li><a class="reference internal" href="#significant-features-by-labels">Significant features by labels</a></li>
<li><a class="reference internal" href="#misclassified-examples">Misclassified examples</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="morf_tables.html"
                        title="previous chapter">Tables</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../api/clausesegmenter.html"
                        title="next chapter">estnltk.clausesegmenter module</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/tutorials/textclassifier.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../api/clausesegmenter.html" title="estnltk.clausesegmenter module"
             >next</a> |</li>
        <li class="right" >
          <a href="morf_tables.html" title="Tables"
             >previous</a> |</li>
        <li><a href="../index.html">estnltk 1.3 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2015, University of Tartu.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>