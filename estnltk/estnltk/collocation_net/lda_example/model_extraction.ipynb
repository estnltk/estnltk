{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of LDA model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/collocation_net/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame, Series \n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel as lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.  Gensim corpus format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A standard workflow to define corpus\n",
    "\n",
    "Let us start with simple sentences and convert them into token vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial set of documents\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "display(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a token dictionary, which we use in final integer-encoded tokenisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "display(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a integer-encoding for the words with gensim library and encode a single text as an example. A corpus data-type is list or sequence of such encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary of encodings\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "\n",
      "Encoding of 'Human human computer interaction' as word-count pairs\n",
      "[(0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary\n",
    "dictionary = Dictionary(texts)\n",
    "print(\"Dictionary of encodings\")\n",
    "print(dictionary.token2id)\n",
    "print(\"\")\n",
    "\n",
    "# Encoding with the dictionary\n",
    "new_doc = \"Human human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(\"Encoding of '{}' as word-count pairs\".format(new_doc))\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a document  for collocations?\n",
    "\n",
    "* Recall that for adjective-noun collocations each document corresponds to a noun.\n",
    "* A document contains adjectives that form collocation pairs with the noun.\n",
    "* This information is stored as dataframe.\n",
    "\n",
    "Hence we can do our own manual converter from dataframe to collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"../data/df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2corpus(x: DataFrame):\n",
    "    return [list(enumerate(row)) for _, row in x.iterrows()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe2idmapping(x: DataFrame):\n",
    "    return {key: value for key, value in enumerate(x.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eelmine</th>\n",
       "      <th>järgmine</th>\n",
       "      <th>viimane</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aasta</th>\n",
       "      <td>53250</td>\n",
       "      <td>38107</td>\n",
       "      <td>24410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aeg</th>\n",
       "      <td>75</td>\n",
       "      <td>65</td>\n",
       "      <td>27915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>määrus</th>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        eelmine  järgmine  viimane\n",
       "aasta     53250     38107    24410\n",
       "aeg          75        65    27915\n",
       "määrus       16       110       17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 53250), (1, 38107), (2, 24410)],\n",
       " [(0, 75), (1, 65), (2, 27915)],\n",
       " [(0, 16), (1, 110), (2, 17)]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{0: 'eelmine', 1: 'järgmine', 2: 'viimane'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = data.iloc[:3, :3]\n",
    "display(tmp)\n",
    "display(dataframe2corpus(tmp))\n",
    "display(dataframe2idmapping(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Gensim call "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the parameter `alpha` to `auto` the implementations finds the best prior distribution for topics as well. There are other settings but these keep the initial topic distribution, you can see it from the gensim code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.206*\"käesolev\" + 0.111*\"kogu\" + 0.071*\"viimane\" + 0.062*\"uus\" + 0.041*\"lugupeetud\" + 0.031*\"pikk\" + 0.027*\"muudetud\" + 0.027*\"vaba\" + 0.022*\"hea\" + 0.021*\"õige\"'),\n",
       " (1,\n",
       "  '0.178*\"eelmine\" + 0.143*\"järgmine\" + 0.093*\"viimane\" + 0.073*\"hea\" + 0.042*\"uus\" + 0.041*\"käesolev\" + 0.035*\"tulev\" + 0.031*\"suur\" + 0.029*\"juriidiline\" + 0.022*\"läinud\"'),\n",
       " (2,\n",
       "  '0.182*\"suur\" + 0.082*\"kohalik\" + 0.060*\"viimane\" + 0.049*\"uus\" + 0.045*\"noor\" + 0.034*\"lugupeetud\" + 0.029*\"hea\" + 0.028*\"praegune\" + 0.024*\"väike\" + 0.022*\"järgmine\"'),\n",
       " (3,\n",
       "  '0.128*\"uus\" + 0.081*\"avalik\" + 0.077*\"hea\" + 0.076*\"viimane\" + 0.054*\"kogu\" + 0.032*\"isiklik\" + 0.028*\"pikk\" + 0.027*\"poliitiline\" + 0.026*\"praegune\" + 0.025*\"õige\"'),\n",
       " (4,\n",
       "  '0.105*\"viimane\" + 0.100*\"järgmine\" + 0.057*\"ettenähtud\" + 0.049*\"terve\" + 0.048*\"vajalik\" + 0.048*\"eelmine\" + 0.046*\"kohalik\" + 0.036*\"uus\" + 0.034*\"külm\" + 0.032*\"üksikasjalik\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = data.iloc[:100, :100]\n",
    "corpus = dataframe2corpus(tmp)\n",
    "idmapping = dataframe2idmapping(tmp)\n",
    "\n",
    "ldamodel = lda(corpus, num_topics=5, id2word=idmapping, alpha='auto')\n",
    "assert ldamodel.optimize_alpha, \"The training does look for optimal alpha\"\n",
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Proper extraction of model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix of word probabilities for each topic $\\boldsymbol{\\beta}$ and prior probability distribution for topics $\\boldsymbol{\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eelmine</th>\n",
       "      <td>0.018361</td>\n",
       "      <td>0.177570</td>\n",
       "      <td>0.014753</td>\n",
       "      <td>0.019628</td>\n",
       "      <td>0.047648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>järgmine</th>\n",
       "      <td>0.020323</td>\n",
       "      <td>0.143423</td>\n",
       "      <td>0.022139</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.100096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viimane</th>\n",
       "      <td>0.071203</td>\n",
       "      <td>0.093035</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>0.104720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>käesolev</th>\n",
       "      <td>0.206296</td>\n",
       "      <td>0.040864</td>\n",
       "      <td>0.007287</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.026075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kogu</th>\n",
       "      <td>0.110683</td>\n",
       "      <td>0.012068</td>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.054339</td>\n",
       "      <td>0.023873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>õigusjärgne</th>\n",
       "      <td>0.004404</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kõva</th>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eraõiguslik</th>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>võrdne</th>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.000585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kinnitamata</th>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.003601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4\n",
       "eelmine      0.018361  0.177570  0.014753  0.019628  0.047648\n",
       "järgmine     0.020323  0.143423  0.022139  0.016380  0.100096\n",
       "viimane      0.071203  0.093035  0.059599  0.075985  0.104720\n",
       "käesolev     0.206296  0.040864  0.007287  0.010630  0.026075\n",
       "kogu         0.110683  0.012068  0.022004  0.054339  0.023873\n",
       "...               ...       ...       ...       ...       ...\n",
       "õigusjärgne  0.004404  0.000118  0.000238  0.000534  0.000008\n",
       "kõva         0.000423  0.000722  0.006294  0.001153  0.000275\n",
       "eraõiguslik  0.000205  0.003066  0.000507  0.000259  0.000397\n",
       "võrdne       0.001321  0.002863  0.003406  0.004784  0.000585\n",
       "kinnitamata  0.000776  0.000454  0.001633  0.000745  0.003601\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.183308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.207601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.235056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.085477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prior\n",
       "0  0.183308\n",
       "1  0.207601\n",
       "2  0.250066\n",
       "3  0.235056\n",
       "4  0.085477"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = DataFrame(ldamodel.get_topics()).rename(columns=idmapping).transpose()\n",
    "display(beta)\n",
    "\n",
    "alpha = DataFrame(ldamodel.alpha, columns=['prior'])\n",
    "display(alpha)\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump([alpha, beta], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Notes on training\n",
    "\n",
    "LDA implementation of gensim is quite similar to sklearn implementation. However there is a difference in the perplecity function. The functions are different but the monotonpusly related\n",
    "\n",
    "* https://stackoverflow.com/questions/40524768/perplexity-comparision-issue-in-sklearn-lda-vs-gensim-lda\n",
    "*  https://github.com/RaRe-Technologies/gensim/issues/457"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
